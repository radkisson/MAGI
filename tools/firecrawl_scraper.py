"""
FireCrawl Scraping Tool for Open WebUI

This tool connects the Cortex (Open WebUI) to the Sensorium's Digestion (FireCrawl),
allowing RIN to extract clean content from complex JavaScript-heavy websites.

Supports both:
- Self-hosted Firecrawl (running in Docker)
- Firecrawl Cloud API (https://api.firecrawl.dev)
"""

import os
import requests
from typing import Callable, Any
from pydantic import BaseModel, Field


class Valves(BaseModel):
    """Configuration valves for FireCrawl integration (auto-loaded from .env)"""

    # Note: default_factory with lambda is required for runtime environment variable loading.
    # This ensures the Valves check environment variables when instantiated, not at import time.
    FIRECRAWL_API_KEY: str = Field(
        default_factory=lambda: os.getenv("FIRECRAWL_API_KEY", ""),
        description="FireCrawl API Key (auto-loaded from .env or set manually)"
    )
    FIRECRAWL_API_URL: str = Field(
        default_factory=lambda: os.getenv("FIRECRAWL_API_URL", "http://firecrawl:3002"),
        description="FireCrawl API URL (default: self-hosted Docker service)"
    )


class Tools:
    """Open WebUI Tool: Web Page Scraping via FireCrawl"""

    def __init__(self):
        self.valves = Valves()

    def scrape_webpage(
        self,
        url: str,
        __user__: dict = {},
        __event_emitter__: Callable[[dict], Any] = None,
    ) -> str:
        """
        Scrape and extract content from a webpage using FireCrawl (The Sensorium's Digestion)

        This tool uses a headless browser to navigate JavaScript-heavy sites and
        converts them into clean Markdown format optimized for LLM consumption.
        FireCrawl handles dynamic content, SPAs, and complex web applications.

        Args:
            url: The webpage URL to scrape
            __user__: User context (provided by Open WebUI)
            __event_emitter__: Event emitter for streaming results (provided by Open WebUI)

        Returns:
            Clean Markdown content extracted from the webpage
        """

        # Validate API key
        if not self.valves.FIRECRAWL_API_KEY:
            error_msg = (
                "‚ùå FireCrawl API key not configured.\n\n"
                "To use FireCrawl:\n"
                "1. Check your .env file for FIRECRAWL_API_KEY\n"
                "2. For self-hosted: Key is auto-generated by start.sh\n"
                "3. For cloud API: Get key from https://firecrawl.dev\n"
                "4. Restart RIN: ./start.sh\n"
            )

            if __event_emitter__:
                __event_emitter__(
                    {
                        "type": "status",
                        "data": {"description": error_msg, "done": True},
                    }
                )

            return error_msg

        if __event_emitter__:
            __event_emitter__(
                {
                    "type": "status",
                    "data": {
                        "description": "üî• Scraping webpage with FireCrawl...",
                        "done": False,
                    },
                }
            )

        try:
            # Request FireCrawl to scrape the URL
            response = requests.post(
                f"{self.valves.FIRECRAWL_API_URL}/v0/scrape",
                json={
                    "url": url,
                    "formats": ["markdown"],
                },
                headers={
                    "Authorization": f"Bearer {self.valves.FIRECRAWL_API_KEY}",
                },
                timeout=30,
            )
            response.raise_for_status()

            result = response.json()

            if __event_emitter__:
                __event_emitter__(
                    {
                        "type": "status",
                        "data": {
                            "description": "‚úÖ Content extracted successfully",
                            "done": True,
                        },
                    }
                )

            # Extract markdown content
            if result.get("success") and result.get("data"):
                markdown_content = result["data"].get("markdown", "")
                metadata = result["data"].get("metadata", {})

                if markdown_content:
                    return (
                        f"# Scraped Content from: {url}\n\n"
                        f"**Title**: {metadata.get('title', 'N/A')}\n"
                        f"**Source**: {metadata.get('sourceURL', url)}\n\n"
                        f"---\n\n"
                        f"{markdown_content}"
                    )
                else:
                    return f"No content could be extracted from {url}"
            else:
                error = result.get("error", "Unknown error")
                return f"Scraping failed: {error}"

        except requests.exceptions.RequestException as e:
            error_msg = f"Error connecting to FireCrawl: {str(e)}"

            if __event_emitter__:
                __event_emitter__(
                    {
                        "type": "status",
                        "data": {"description": f"‚ùå {error_msg}", "done": True},
                    }
                )

            return f"Scraping failed: {error_msg}\n\nNote: Ensure FireCrawl service is running (docker-compose up -d)"

    def crawl_website(
        self,
        url: str,
        max_pages: int = 10,
        __user__: dict = {},
        __event_emitter__: Callable[[dict], Any] = None,
    ) -> str:
        """
        Crawl multiple pages from a website using FireCrawl

        This tool crawls a website starting from the given URL, following links
        and extracting content from multiple pages. Useful for gathering
        comprehensive information from a site.

        Args:
            url: The starting URL to crawl
            max_pages: Maximum number of pages to crawl (default: 10)
            __user__: User context (provided by Open WebUI)
            __event_emitter__: Event emitter for streaming results (provided by Open WebUI)

        Returns:
            Combined Markdown content from all crawled pages
        """

        # Validate API key
        if not self.valves.FIRECRAWL_API_KEY:
            error_msg = (
                "‚ùå FireCrawl API key not configured.\n\n"
                "To use FireCrawl:\n"
                "1. Check your .env file for FIRECRAWL_API_KEY\n"
                "2. For self-hosted: Key is auto-generated by start.sh\n"
                "3. For cloud API: Get key from https://firecrawl.dev\n"
                "4. Restart RIN: ./start.sh\n"
            )

            if __event_emitter__:
                __event_emitter__(
                    {
                        "type": "status",
                        "data": {"description": error_msg, "done": True},
                    }
                )

            return error_msg

        if __event_emitter__:
            __event_emitter__(
                {
                    "type": "status",
                    "data": {
                        "description": f"üî• Starting website crawl (max {max_pages} pages)...",
                        "done": False,
                    },
                }
            )

        try:
            # Request FireCrawl to crawl the website
            response = requests.post(
                f"{self.valves.FIRECRAWL_API_URL}/v0/crawl",
                json={
                    "url": url,
                    "limit": max_pages,
                    "scrapeOptions": {
                        "formats": ["markdown"],
                    },
                },
                headers={
                    "Authorization": f"Bearer {self.valves.FIRECRAWL_API_KEY}",
                },
                timeout=60,
            )
            response.raise_for_status()

            result = response.json()

            if __event_emitter__:
                __event_emitter__(
                    {
                        "type": "status",
                        "data": {
                            "description": "‚úÖ Crawl completed",
                            "done": True,
                        },
                    }
                )

            if result.get("success") and result.get("data"):
                pages = result["data"]

                # Combine all pages
                combined_content = f"# Website Crawl: {url}\n\n"
                combined_content += f"Crawled {len(pages)} pages\n\n"
                combined_content += "---\n\n"

                for idx, page in enumerate(pages, 1):
                    markdown = page.get("markdown", "")
                    metadata = page.get("metadata", {})

                    combined_content += f"## Page {idx}: {metadata.get('title', 'Untitled')}\n"
                    combined_content += f"URL: {metadata.get('sourceURL', 'N/A')}\n\n"
                    combined_content += markdown + "\n\n---\n\n"

                return combined_content
            else:
                error = result.get("error", "Unknown error")
                return f"Crawling failed: {error}"

        except requests.exceptions.RequestException as e:
            error_msg = f"Error connecting to FireCrawl: {str(e)}"

            if __event_emitter__:
                __event_emitter__(
                    {
                        "type": "status",
                        "data": {"description": f"‚ùå {error_msg}", "done": True},
                    }
                )

            return f"Crawling failed: {error_msg}\n\nNote: Ensure FireCrawl service is running (docker-compose up -d)"
