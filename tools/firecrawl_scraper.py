"""
FireCrawl Scraping Tool for Open WebUI

This tool connects the Cortex (Open WebUI) to the Sensorium's Digestion (FireCrawl),
allowing RIN to extract clean content from complex JavaScript-heavy websites.

Supports both:
- Self-hosted Firecrawl (running in Docker)
- Firecrawl Cloud API (https://api.firecrawl.dev)
"""

import os
import json
import requests
from typing import Callable, Any, Optional, List
from pydantic import BaseModel, Field
from threading import Lock


class Valves(BaseModel):
    """Configuration valves for FireCrawl integration (auto-loaded from .env)"""

    # Note: default_factory with lambda is required for runtime environment variable loading.
    # This ensures the Valves check environment variables when instantiated, not at import time.
    FIRECRAWL_API_KEY: str = Field(
        default_factory=lambda: os.getenv("FIRECRAWL_API_KEY", ""),
        description="FireCrawl API Key (auto-loaded from .env or set manually)"
    )
    FIRECRAWL_API_URL: str = Field(
        default_factory=_get_firecrawl_url,
        description="FireCrawl API URL (default: self-hosted Docker service, auto-detects HTTP/HTTPS)"
    )
    # Improvement 1: Adjustable timeouts and limits
    REQUEST_TIMEOUT: int = Field(
        default=60,
        description="Timeout in seconds for scraping requests"
    )
    MAX_CONTENT_LENGTH: int = Field(
        default=20000,
        description="Max characters to return per scrape to prevent context overflow"
    )
    CACHE_MAX_SIZE: int = Field(
        default=100,
        description="Maximum number of cached results to store in memory"
    )
    CRAWL_TIMEOUT_BUFFER: int = Field(
        default=60,
        description="Additional timeout (in seconds) to add for crawl operations beyond REQUEST_TIMEOUT"
    )


def _get_firecrawl_url() -> str:
    """Get FireCrawl URL, auto-detecting HTTP or HTTPS based on configuration."""
    from .utils import get_service_url
    return get_service_url("firecrawl", 3002, check_env_var="FIRECRAWL_API_URL")


class Tools:
    """Open WebUI Tool: Web Page Scraping via FireCrawl"""

    def __init__(self):
        self.valves = Valves()
        # Improvement 3: Simple in-memory cache with thread safety
        self._cache = {}
        self._cache_lock = Lock()
        self._cache_order = []  # Track insertion order for LRU eviction

    def _get_from_cache(self, key: str) -> Optional[str]:
        """Thread-safe cache retrieval"""
        with self._cache_lock:
            if key in self._cache:
                # Move to end (most recently used)
                self._cache_order.remove(key)
                self._cache_order.append(key)
                return self._cache[key]
        return None

    def _add_to_cache(self, key: str, value: str) -> None:
        """Thread-safe cache addition with LRU eviction"""
        with self._cache_lock:
            # Remove oldest entries if cache is full
            while len(self._cache) >= self.valves.CACHE_MAX_SIZE:
                if self._cache_order:
                    oldest = self._cache_order.pop(0)
                    del self._cache[oldest]
            
            # Add new entry
            if key in self._cache:
                self._cache_order.remove(key)
            self._cache[key] = value
            self._cache_order.append(key)

    def _truncate_content(self, content: str, source: str) -> str:
        """Helper to safely truncate content"""
        limit = self.valves.MAX_CONTENT_LENGTH
        if len(content) <= limit:
            return content
        
        return (
            f"{content[:limit]}\n\n"
            f"> ‚ö†Ô∏è **System Note**: Content from {source} was truncated because it exceeded "
            f"{limit} characters. The AI has only read the first part."
        )

    def scrape_webpage(
        self,
        url: str,
        __user__: dict = {},
        __event_emitter__: Callable[[dict], Any] = None,
    ) -> str:
        """
        Scrape and extract content from a webpage using FireCrawl (The Sensorium's Digestion)

        This tool uses a headless browser to navigate JavaScript-heavy sites and
        converts them into clean Markdown format optimized for LLM consumption.
        FireCrawl handles dynamic content, SPAs, and complex web applications.

        Args:
            url: The webpage URL to scrape
            __user__: User context (provided by Open WebUI)
            __event_emitter__: Event emitter for streaming results (provided by Open WebUI)

        Returns:
            Clean Markdown content extracted from the webpage
        """

        # Validate API key
        if not self.valves.FIRECRAWL_API_KEY:
            error_msg = (
                "‚ùå FireCrawl API key not configured.\n\n"
                "To use FireCrawl:\n"
                "1. Check your .env file for FIRECRAWL_API_KEY\n"
                "2. For self-hosted: Key is auto-generated by start.sh\n"
                "3. For cloud API: Get key from https://firecrawl.dev\n"
                "4. Restart RIN: ./start.sh\n"
            )

            if __event_emitter__:
                __event_emitter__(
                    {
                        "type": "status",
                        "data": {"description": error_msg, "done": True},
                    }
                )

            return error_msg

        # Check Cache
        cached_result = self._get_from_cache(url)
        if cached_result:
            if __event_emitter__:
                __event_emitter__(
                    {
                        "type": "status",
                        "data": {"description": "‚ö° Retrieved from cache", "done": True}
                    }
                )
            return cached_result

        if __event_emitter__:
            __event_emitter__(
                {
                    "type": "status",
                    "data": {
                        "description": "üî• Scraping webpage with FireCrawl...",
                        "done": False,
                    },
                }
            )

        try:
            # Request FireCrawl to scrape the URL
            response = requests.post(
                f"{self.valves.FIRECRAWL_API_URL}/v0/scrape",
                json={
                    "url": url,
                    "formats": ["markdown"],
                },
                headers={
                    "Authorization": f"Bearer {self.valves.FIRECRAWL_API_KEY}",
                },
                timeout=self.valves.REQUEST_TIMEOUT,
            )
            response.raise_for_status()

            try:
                result = response.json()
            except json.JSONDecodeError:
                if __event_emitter__:
                    __event_emitter__(
                        {
                            "type": "status",
                            "data": {
                                "description": "‚ùå Invalid JSON response from FireCrawl",
                                "done": True,
                            },
                        }
                    )
                return (
                    f"‚ùå FireCrawl returned invalid JSON response for {url}\n\n"
                    f"Response: {response.text[:500]}\n\n"
                    f"This may indicate a FireCrawl service error. Check the logs."
                )
            
            # Check if response is empty or malformed
            if not result or result == {}:
                if __event_emitter__:
                    __event_emitter__(
                        {
                            "type": "status",
                            "data": {
                                "description": "‚ö†Ô∏è Received empty response from FireCrawl",
                                "done": True,
                            },
                        }
                    )
                return (
                    f"‚ö†Ô∏è FireCrawl returned an empty response for {url}\n\n"
                    f"This may indicate:\n"
                    f"1. The FireCrawl service is not properly configured\n"
                    f"2. The URL may be inaccessible or blocked\n"
                    f"3. The FireCrawl API may be experiencing issues\n\n"
                    f"Try:\n"
                    f"- Verify FireCrawl is running: `docker ps | grep firecrawl`\n"
                    f"- Check FireCrawl logs: `docker logs rin-firecrawl`\n"
                    f"- Test with a simpler URL"
                )

            if __event_emitter__:
                __event_emitter__(
                    {
                        "type": "status",
                        "data": {
                            "description": "‚úÖ Content extracted successfully",
                            "done": True,
                        },
                    }
                )

            # Extract markdown content
            if result.get("success") and result.get("data"):
                markdown_content = result["data"].get("markdown", "")
                metadata = result["data"].get("metadata", {})

                if markdown_content:
                    # Apply Truncation
                    markdown_content = self._truncate_content(markdown_content, url)
                    
                    formatted_output = (
                        f"# Scraped Content from: {url}\n\n"
                        f"**Title**: {metadata.get('title', 'N/A')}\n"
                        f"**Source**: {metadata.get('sourceURL', url)}\n\n"
                        f"---\n\n"
                        f"{markdown_content}"
                    )
                    
                    # Save to Cache
                    self._add_to_cache(url, formatted_output)
                    
                    return formatted_output
                else:
                    return f"No content could be extracted from {url}"
            else:
                error = result.get("error", "Unknown error")
                return f"Scraping failed: {error}"

        except requests.exceptions.RequestException as e:
            error_msg = f"Error connecting to FireCrawl: {str(e)}"

            if __event_emitter__:
                __event_emitter__(
                    {
                        "type": "status",
                        "data": {"description": f"‚ùå {error_msg}", "done": True},
                    }
                )

            return f"Scraping failed: {error_msg}\n\nNote: Ensure FireCrawl service is running (docker-compose up -d)"

    def crawl_website(
        self,
        url: str,
        max_pages: int = 10,
        include_paths: Optional[List[str]] = None,
        exclude_paths: Optional[List[str]] = None,
        __user__: dict = {},
        __event_emitter__: Callable[[dict], Any] = None,
    ) -> str:
        """
        Crawl multiple pages from a website using FireCrawl

        This tool crawls a website starting from the given URL, following links
        and extracting content from multiple pages. Useful for gathering
        comprehensive information from a site.

        Args:
            url: The starting URL to crawl
            max_pages: Maximum number of pages to crawl (default: 10)
            include_paths: Only crawl URLs matching these patterns (e.g. ["/docs/*"])
            exclude_paths: Skip URLs matching these patterns (e.g. ["/blog/*"])
            __user__: User context (provided by Open WebUI)
            __event_emitter__: Event emitter for streaming results (provided by Open WebUI)

        Returns:
            Combined Markdown content from all crawled pages
        """

        # Validate API key
        if not self.valves.FIRECRAWL_API_KEY:
            error_msg = (
                "‚ùå FireCrawl API key not configured.\n\n"
                "To use FireCrawl:\n"
                "1. Check your .env file for FIRECRAWL_API_KEY\n"
                "2. For self-hosted: Key is auto-generated by start.sh\n"
                "3. For cloud API: Get key from https://firecrawl.dev\n"
                "4. Restart RIN: ./start.sh\n"
            )

            if __event_emitter__:
                __event_emitter__(
                    {
                        "type": "status",
                        "data": {"description": error_msg, "done": True},
                    }
                )

            return error_msg

        if __event_emitter__:
            status_msg = f"üî• Starting website crawl (max {max_pages} pages)"
            if include_paths:
                status_msg += f" focusing on {include_paths}"
            
            __event_emitter__(
                {
                    "type": "status",
                    "data": {
                        "description": status_msg,
                        "done": False,
                    },
                }
            )

        try:
            # Request FireCrawl to crawl the website
            payload = {
                "url": url,
                "limit": max_pages,
                "scrapeOptions": {
                    "formats": ["markdown"],
                },
            }

            # Add filters if the AI requested them
            if include_paths:
                payload["includePaths"] = include_paths
            if exclude_paths:
                payload["excludePaths"] = exclude_paths

            response = requests.post(
                f"{self.valves.FIRECRAWL_API_URL}/v0/crawl",
                json=payload,
                headers={
                    "Authorization": f"Bearer {self.valves.FIRECRAWL_API_KEY}",
                },
                timeout=self.valves.REQUEST_TIMEOUT + self.valves.CRAWL_TIMEOUT_BUFFER,
            )
            response.raise_for_status()

            try:
                result = response.json()
            except json.JSONDecodeError:
                if __event_emitter__:
                    __event_emitter__(
                        {
                            "type": "status",
                            "data": {
                                "description": "‚ùå Invalid JSON response from FireCrawl",
                                "done": True,
                            },
                        }
                    )
                return (
                    f"‚ùå FireCrawl returned invalid JSON response for crawl of {url}\n\n"
                    f"Response: {response.text[:500]}\n\n"
                    f"This may indicate a FireCrawl service error. Check the logs."
                )
            
            # Check if response is empty or malformed
            if not result or result == {}:
                if __event_emitter__:
                    __event_emitter__(
                        {
                            "type": "status",
                            "data": {
                                "description": "‚ö†Ô∏è Received empty response from FireCrawl",
                                "done": True,
                            },
                        }
                    )
                return (
                    f"‚ö†Ô∏è FireCrawl returned an empty response for crawl of {url}\n\n"
                    f"This may indicate:\n"
                    f"1. The FireCrawl service is not properly configured\n"
                    f"2. The website may be blocking crawlers\n"
                    f"3. The FireCrawl API may be experiencing issues\n\n"
                    f"Try:\n"
                    f"- Verify FireCrawl is running: `docker ps | grep firecrawl`\n"
                    f"- Check FireCrawl logs: `docker logs rin-firecrawl`\n"
                    f"- Try scraping a single page first with `scrape_webpage()`"
                )

            if __event_emitter__:
                __event_emitter__(
                    {
                        "type": "status",
                        "data": {
                            "description": "‚úÖ Crawl completed",
                            "done": True,
                        },
                    }
                )

            if result.get("success") and result.get("data"):
                pages = result["data"]

                # Combine all pages
                combined_content = f"# Website Crawl: {url}\n\n"
                combined_content += f"Crawled {len(pages)} pages\n\n"
                combined_content += "---\n\n"

                for idx, page in enumerate(pages, 1):
                    markdown = page.get("markdown", "")
                    metadata = page.get("metadata", {})

                    combined_content += f"## Page {idx}: {metadata.get('title', 'Untitled')}\n"
                    combined_content += f"URL: {metadata.get('sourceURL', 'N/A')}\n\n"
                    combined_content += markdown + "\n\n---\n\n"

                # Ensure combined result doesn't explode context
                # We truncate the FINAL combined string to ensure the total package fits
                return self._truncate_content(combined_content, f"Crawl of {url}")
            else:
                error = result.get("error", "Unknown error")
                return f"Crawling failed: {error}"

        except requests.exceptions.RequestException as e:
            error_msg = f"Error connecting to FireCrawl: {str(e)}"

            if __event_emitter__:
                __event_emitter__(
                    {
                        "type": "status",
                        "data": {"description": f"‚ùå {error_msg}", "done": True},
                    }
                )

            return f"Crawling failed: {error_msg}\n\nNote: Ensure FireCrawl service is running (docker-compose up -d)"
