"""
Qdrant Memory Tool for Open WebUI

This tool connects the Cortex (Open WebUI) to the Memory (Qdrant),
allowing RIN to store and retrieve information with RAG capabilities.
"""

import requests
from typing import Callable, Any, List
import json
import uuid
import time
from sentence_transformers import SentenceTransformer


class Tools:
    """Open WebUI Tool: Long-Term Memory via Qdrant Vector Database"""

    def __init__(self):
        self.qdrant_url = "http://qdrant:6333"
        self.collection_name = "rin_memory"
        self.embedding_model = None
        self.embedding_dim = 768  # Standard dimension for many models

    def _get_embedding_model(self):
        """
        Lazily initialize and return the embedding model.
        Uses sentence-transformers with a 768-dimensional model.
        """
        if self.embedding_model is None:
            try:
                # Using all-mpnet-base-v2: 768 dimensions, good quality, reasonable speed
                self.embedding_model = SentenceTransformer('all-mpnet-base-v2')
            except Exception as e:
                raise Exception(f"Failed to load embedding model: {str(e)}")
        return self.embedding_model

    def store_memory(
        self,
        content: str,
        metadata: dict = None,
        __user__: dict = {},
        __event_emitter__: Callable[[dict], Any] = None,
    ) -> str:
        """
        Store information in RIN's long-term memory (Qdrant Vector Database)

        This tool allows RIN to remember facts, conversations, and documents
        for future recall using RAG (Retrieval Augmented Generation).

        Args:
            content: The text content to store in memory
            metadata: Optional metadata (tags, source, timestamp, etc.)
            __user__: User context (provided by Open WebUI)
            __event_emitter__: Event emitter for streaming results (provided by Open WebUI)

        Returns:
            Confirmation message with memory ID
        """

        if __event_emitter__:
            __event_emitter__(
                {
                    "type": "status",
                    "data": {
                        "description": "üíæ Storing in long-term memory...",
                        "done": False,
                    },
                }
            )

        try:
            # First, ensure collection exists
            self._ensure_collection_exists()

            # Generate embedding using sentence-transformers
            model = self._get_embedding_model()
            embedding = model.encode(content).tolist()

            # Generate unique ID using UUID to avoid collisions
            memory_id = str(uuid.uuid4())

            # Prepare metadata (create a new dict to avoid modifying the input)
            prepared_metadata = dict(metadata) if metadata else {}
            
            # Add user context to metadata
            if __user__:
                prepared_metadata["user_id"] = __user__.get("id", "unknown")
                prepared_metadata["user_name"] = __user__.get("name", "unknown")
            
            # Add timestamp
            prepared_metadata["timestamp"] = time.time()

            # Store in Qdrant using upsert
            upsert_payload = {
                "points": [
                    {
                        "id": memory_id,
                        "vector": embedding,
                        "payload": {
                            "content": content,
                            "metadata": prepared_metadata
                        }
                    }
                ]
            }

            response = requests.put(
                f"{self.qdrant_url}/collections/{self.collection_name}/points",
                json=upsert_payload,
                timeout=10,
            )

            if response.status_code not in [200, 201]:
                raise Exception(
                    f"Failed to store in Qdrant: {response.status_code} - {response.text}"
                )

            if __event_emitter__:
                __event_emitter__(
                    {
                        "type": "status",
                        "data": {
                            "description": "‚úÖ Memory stored successfully",
                            "done": True,
                        },
                    }
                )

            return (
                f"‚úÖ Stored in RIN's long-term memory\n\n"
                f"**Memory ID**: {memory_id}\n"
                f"**Content**: {content[:200]}{'...' if len(content) > 200 else ''}\n\n"
                f"This information can now be recalled using semantic search."
            )

        except Exception as e:
            error_msg = f"Error storing memory: {str(e)}"

            if __event_emitter__:
                __event_emitter__(
                    {
                        "type": "status",
                        "data": {"description": f"‚ùå {error_msg}", "done": True},
                    }
                )

            return f"Memory storage failed: {error_msg}"

    def recall_memory(
        self,
        query: str,
        limit: int = 5,
        __user__: dict = {},
        __event_emitter__: Callable[[dict], Any] = None,
    ) -> str:
        """
        Recall information from RIN's long-term memory using semantic search

        This tool performs RAG (Retrieval Augmented Generation) by searching
        the vector database for semantically similar content to the query.
        This prevents hallucination by providing factual context.

        Args:
            query: What to search for in memory
            limit: Maximum number of results to return (default: 5)
            __user__: User context (provided by Open WebUI)
            __event_emitter__: Event emitter for streaming results (provided by Open WebUI)

        Returns:
            Relevant memories from the vector database
        """

        if __event_emitter__:
            __event_emitter__(
                {
                    "type": "status",
                    "data": {
                        "description": "üß† Searching long-term memory...",
                        "done": False,
                    },
                }
            )

        try:
            # Generate query embedding
            model = self._get_embedding_model()
            query_embedding = model.encode(query).tolist()

            # Search Qdrant for similar vectors
            search_payload = {
                "vector": query_embedding,
                "limit": limit,
                "with_payload": True
            }

            response = requests.post(
                f"{self.qdrant_url}/collections/{self.collection_name}/points/search",
                json=search_payload,
                timeout=10,
            )

            if response.status_code != 200:
                raise Exception(
                    f"Failed to search Qdrant: {response.status_code} - {response.text}"
                )

            results = response.json().get("result", [])

            if __event_emitter__:
                __event_emitter__(
                    {
                        "type": "status",
                        "data": {
                            "description": "‚úÖ Memory search completed",
                            "done": True,
                        },
                    }
                )

            # Format results
            if not results:
                return (
                    f"# Memory Recall: '{query}'\n\n"
                    f"No memories found matching this query.\n"
                    f"Store memories using the `store_memory` tool."
                )

            # Build response with results
            response_text = f"# Memory Recall: '{query}'\n\n"
            response_text += f"Found {len(results)} relevant memories:\n\n"
            
            for idx, result in enumerate(results, 1):
                score = result.get("score", 0)
                payload = result.get("payload", {})
                content = payload.get("content", "")
                metadata = payload.get("metadata", {})
                
                response_text += f"## Memory {idx} (Similarity: {score:.3f})\n"
                response_text += f"{content}\n"
                
                if metadata:
                    response_text += f"\n*Metadata: {json.dumps(metadata, indent=2)}*\n"
                
                response_text += "\n---\n\n"

            return response_text

        except Exception as e:
            error_msg = f"Error recalling memory: {str(e)}"

            if __event_emitter__:
                __event_emitter__(
                    {
                        "type": "status",
                        "data": {"description": f"‚ùå {error_msg}", "done": True},
                    }
                )

            return f"Memory recall failed: {error_msg}"

    def _ensure_collection_exists(self):
        """
        Ensure the Qdrant collection exists, create if needed.

        This is critical on first deployment - without this check, the memory
        tool will crash when trying to upsert data into a non-existent collection.
        """
        try:
            # Check if collection exists
            response = requests.get(
                f"{self.qdrant_url}/collections/{self.collection_name}",
                timeout=5,
            )

            if response.status_code == 404:
                # Collection doesn't exist - create it
                create_response = requests.put(
                    f"{self.qdrant_url}/collections/{self.collection_name}",
                    json={
                        "vectors": {
                            "size": 768,  # Dimension for all-mpnet-base-v2 model
                            "distance": "Cosine",  # Cosine similarity for semantic search
                        }
                    },
                    timeout=5,
                )

                if create_response.status_code not in [200, 201]:
                    raise Exception(
                        f"Failed to create collection: {create_response.status_code}"
                    )

            elif response.status_code != 200:
                raise Exception(
                    f"Failed to check collection status: {response.status_code}"
                )

        except requests.exceptions.ConnectionError:
            raise Exception(
                "Cannot connect to Qdrant. Ensure service is running (docker-compose up -d)"
            )
        except requests.exceptions.Timeout:
            raise Exception(
                "Qdrant connection timeout. Service may be starting up or overloaded"
            )
        except Exception as e:
            if "Failed" in str(e) or "Cannot connect" in str(e):
                raise  # Re-raise our custom exceptions
            # For other exceptions, provide generic error
            raise Exception(f"Qdrant initialization failed: {str(e)}")
