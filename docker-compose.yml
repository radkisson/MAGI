services:
  # --- THE CORTEX (Brain) ---
  open-webui:
    # Note: Using 'main' tag for latest features as specified in deployment architecture
    image: ghcr.io/open-webui/open-webui:main
    container_name: rin-cortex
    restart: always
    ports:
      - "${PORT_WEBUI:-3000}:8080"
    volumes:
      - ./data/open-webui:/app/backend/data
      # Mount the tools directly so they are available immediately
      - ./tools:/app/backend/data/tools
    environment:
      - OPENAI_API_BASE_URL=http://litellm:4000
      - ENABLE_RAG_WEB_SEARCH=True
      - SEARXNG_QUERY_URL=http://searxng:8080/search?q=<query>
      # External Senses: API Keys for Tools (Auto-Configuration)
      - FIRECRAWL_API_KEY=${FIRECRAWL_API_KEY}
      - TAVILY_API_KEY=${TAVILY_API_KEY}
    env_file: .env
    depends_on:
      litellm:
        condition: service_healthy
      qdrant:
        condition: service_healthy
      searxng:
        condition: service_healthy
    extra_hosts:
      - "host.docker.internal:host-gateway"

  # --- THE ROUTER ---
  litellm:
    # Note: Using 'main-latest' tag for latest features as specified in deployment architecture
    image: ghcr.io/berriai/litellm:main-latest
    container_name: rin-router
    restart: always
    ports:
      - "${PORT_LITELLM:-4000}:4000"
    volumes:
      - ./config/litellm/config.yaml:/app/config.yaml
      - ./data/litellm:/app/data
    command: [ "--config", "/app/config.yaml", "--port", "4000" ]
    env_file: .env
    depends_on:
      redis:
        condition: service_healthy
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      # Set defaults for OpenRouter webhook headers to prevent NoneType errors
      # These will be overridden if set in .env file
      - OPENROUTER_SITE_URL=${OPENROUTER_SITE_URL:-http://localhost:3000}
      - OPENROUTER_APP_NAME=${OPENROUTER_APP_NAME:-RIN - Rhyzomic Intelligence Node}
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import urllib.request; urllib.request.urlopen('http://localhost:4000/')\" || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  # --- THE SENSORIUM (Vision) ---
  searxng:
    # Note: Requires config/searxng/settings.yml to be created before starting
    # Run ./start.sh to auto-generate configuration, or manually create from settings.yml.example
    image: searxng/searxng:latest
    container_name: rin-vision
    restart: always
    ports:
      - "${PORT_SEARXNG:-8080}:8080"
    volumes:
      - ./config/searxng:/etc/searxng
    env_file: .env
    environment:
      - SEARXNG_BASE_URL=http://localhost:8080
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:8080/ || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  # --- THE DIGESTION ---
  firecrawl:
    image: ghcr.io/firecrawl/firecrawl:latest
    container_name: rin-digestion
    restart: always
    profiles:
      - firecrawl
    ports:
      - "${PORT_FIRECRAWL:-3002}:3002"
    environment:
      - REDIS_URL=redis://redis:6379
      - PLAYWRIGHT_MICROSERVICE_URL=http://playwright-service:3000/scrape
      # Automatically uses the key generated in start.sh
      - TEST_API_KEY=${FIRECRAWL_API_KEY}
      - HOST=0.0.0.0
      - PORT=3002
      - USE_DB_AUTHENTICATION=false
      - NUM_WORKERS_PER_QUEUE=1
      # Disable NUQ database and RabbitMQ setup (prevents Docker-in-Docker errors)
      # These are non-functional placeholder values that prevent automatic container setup
      # Do NOT use these values for actual database connections
      - NUQ_DATABASE_URL=postgresql://disabled:disabled@127.0.0.1:1/disabled
      - NUQ_RABBITMQ_URL=amqp://disabled:disabled@127.0.0.1:1
    depends_on:
      - redis
      - playwright-service

  playwright-service:
    image: ghcr.io/firecrawl/playwright-service:latest
    container_name: rin-browser
    restart: always
    profiles:
      - firecrawl
    environment:
      - PORT=3000

  # --- THE REFLEX (Autonomy) ---
  n8n:
    image: n8nio/n8n:latest
    container_name: rin-reflex-automation
    restart: always
    ports:
      - "${PORT_N8N:-5678}:5678"
    volumes:
      - ./data/n8n:/home/node/.n8n
      - ./workflows:/workflows:ro
    environment:
      - N8N_HOST=0.0.0.0
      - N8N_PORT=5678
      - N8N_PROTOCOL=http
      - WEBHOOK_URL=http://localhost:5678
      - GENERIC_TIMEZONE=UTC
      - N8N_EDITOR_BASE_URL=http://localhost:5678
      - N8N_SECURE_COOKIE=false  # Disable secure cookie for local HTTP development
    env_file: .env
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:5678/healthz || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  # --- THE MCP BRIDGE (Sequential Thinking) ---
  mcp-bridge:
    image: ghcr.io/open-webui/mcpo:main
    container_name: rin-mcp-bridge
    restart: always
    ports:
      - "${PORT_MCP_BRIDGE:-9000}:${PORT_MCP_BRIDGE:-9000}"
    environment:
      - PORT_MCP_BRIDGE=${PORT_MCP_BRIDGE:-9000}
    command:
      - "--port"
      - "${PORT_MCP_BRIDGE:-9000}"
      - "--"
      - "npx"
      - "-y"
      - "@modelcontextprotocol/server-sequential-thinking"
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:${PORT_MCP_BRIDGE:-9000}/openapi.json || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    # This service bridges Model Context Protocol (MCP) tools to Open WebUI.
    # Sequential Thinking forces the AI to use a "Chain of Thought" approach,
    # resulting in deeper, more methodical reasoning for complex queries.

  # --- YOUTUBE TRANSCRIPT BRIDGE ---
  youtube-mcp:
    image: ghcr.io/open-webui/mcpo:main
    container_name: rin-youtube-mcp
    restart: always
    ports:
      - "${PORT_YOUTUBE_MCP:-9001}:${PORT_YOUTUBE_MCP:-9001}"
    environment:
      - PORT_YOUTUBE_MCP=${PORT_YOUTUBE_MCP:-9001}
    command:
      - "--port"
      - "${PORT_YOUTUBE_MCP:-9001}"
      - "--"
      - "npx"
      - "-y"
      - "@sinco-lab/mcp-youtube-transcript"
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:${PORT_YOUTUBE_MCP:-9001}/openapi.json || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    # This service allows the AI to "watch" videos by reading YouTube subtitle tracks.
    # Uses the mcpo image which includes Node.js, npm, and mcpo pre-installed.

  # --- SUPPORT ---
  redis:
    image: redis:alpine
    container_name: rin-nervous-system
    restart: always
    volumes:
      - ./data/redis:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 5s

  qdrant:
    image: qdrant/qdrant
    container_name: rin-memory
    restart: always
    ports:
      - "${PORT_QDRANT:-6333}:6333"
    volumes:
      - ./data/qdrant:/qdrant/storage
    healthcheck:
      test: ["CMD-SHELL", "timeout 1 bash -c '</dev/tcp/localhost/6333' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
