# v1.1 Enhanced Model Support - Smoke Tests

This document provides quick smoke tests to verify that v1.1 features are working correctly after deployment.

## Prerequisites

1. RIN is running: `docker-compose ps` shows all services healthy
2. You have at least one API key configured (OpenAI, Anthropic, or OpenRouter)
3. You can access Open WebUI at http://localhost:3000

## Test 1: Verify Model Discovery

**Goal**: Confirm all configured models appear in Open WebUI

**Steps**:
1. Open http://localhost:3000
2. Start a new chat
3. Click the model dropdown at the top
4. Verify you see all configured models

**Expected Results**:
- Direct API models: `gpt-4o`, `gpt-4o-mini`, `claude-3-5-sonnet`, `claude-3-5-haiku`
- OpenRouter models: `openrouter/gpt-4o`, `openrouter/llama-3.1-405b`, etc.
- Total: 16 models if all API keys are configured

**Troubleshooting**:
```bash
# Check LiteLLM logs
docker-compose logs litellm

# Verify models endpoint
curl http://localhost:4000/models
```

## Test 2: Model Selection and Chat

**Goal**: Verify model switching works and messages are sent correctly

**Steps**:
1. Select `gpt-4o-mini` from the model dropdown
2. Send a test message: "Hello, what model are you?"
3. Wait for response
4. Switch to `claude-3-5-haiku`
5. Send the same message again

**Expected Results**:
- Both models respond appropriately
- Each model identifies itself correctly
- No errors in the UI or console

**Troubleshooting**:
```bash
# Check if LiteLLM received the request
docker-compose logs litellm | grep -i "post /chat"

# Check for API errors
docker-compose logs litellm | grep -i "error"
```

## Test 3: Cost Tracking Database

**Goal**: Verify cost tracking is recording requests

**Steps**:
1. Send 2-3 test messages using any model
2. Wait 10 seconds for database writes
3. Check the database:

```bash
# Connect to the cost tracking database
sqlite3 data/litellm/litellm_cost_tracking.db

# View the schema
.schema

# Check if requests are being logged
SELECT COUNT(*) FROM spend_log;

# View recent requests with costs
SELECT 
    model, 
    ROUND(cost, 6) as cost_usd,
    total_tokens,
    datetime(created_at, 'unixepoch') as timestamp
FROM spend_log 
ORDER BY created_at DESC 
LIMIT 5;

# Calculate total spending by model
SELECT 
    model,
    COUNT(*) as requests,
    ROUND(SUM(cost), 4) as total_cost_usd,
    SUM(total_tokens) as total_tokens
FROM spend_log 
GROUP BY model
ORDER BY total_cost_usd DESC;

# Exit sqlite
.exit
```

**Expected Results**:
- Database exists at `data/litellm/litellm_cost_tracking.db`
- `spend_log` table contains your test requests
- Each request has: model name, cost, token counts, timestamp
- Costs are reasonable (e.g., gpt-4o-mini ~$0.0001-0.001 per message)

**Troubleshooting**:
```bash
# Check if database file exists
ls -lh data/litellm/litellm_cost_tracking.db

# Check LiteLLM database logs
docker-compose logs litellm | grep -i "database\|sqlite"

# Verify database permissions
ls -la data/litellm/
```

## Test 4: OpenRouter Integration

**Goal**: Verify OpenRouter models work correctly

**Prerequisites**: `OPENROUTER_API_KEY` must be set in `.env`

**Steps**:
1. Select an OpenRouter model (e.g., `openrouter/llama-3.1-70b`)
2. Send a test message: "Introduce yourself briefly"
3. Verify response is received

**Expected Results**:
- OpenRouter model responds successfully
- No authentication errors
- Cost is tracked in database

**Troubleshooting**:
```bash
# Check OpenRouter API key is loaded
docker-compose exec litellm env | grep OPENROUTER

# Check for OpenRouter-specific errors
docker-compose logs litellm | grep -i "openrouter"

# Test OpenRouter directly
curl http://localhost:4000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $LITELLM_MASTER_KEY" \
  -d '{
    "model": "openrouter/llama-3.1-70b",
    "messages": [{"role": "user", "content": "Hi"}],
    "max_tokens": 50
  }'
```

## Test 5: Advanced Model Parameters

**Goal**: Verify temperature, top_p, and max_tokens are working

**Steps**:
1. Send a creative writing request to test temperature:
   - Model: `gpt-4o`
   - Message: "Write a creative one-sentence story about a robot"
   - Send this 3 times and observe variation

2. Check the config applies parameters:
```bash
# View model configuration
cat config/litellm/config.yaml | grep -A 5 "model_name: gpt-4o"
```

**Expected Results**:
- Responses show creative variation (due to temperature: 0.7)
- Responses are not too long (due to max_tokens: 4096)
- Config shows temperature, top_p, and max_tokens for all models

**Troubleshooting**:
```bash
# Check if parameters are being sent to API
docker-compose logs litellm | grep -i "temperature\|max_tokens"
```

## Test 6: Fallback Chain (Advanced)

**Goal**: Verify fallback chains activate on primary model failure

**Note**: This is harder to test without actually causing a failure. Instead, we'll verify the configuration:

**Steps**:
1. Review the fallback configuration:
```bash
cat config/litellm/config.yaml | grep -A 20 "fallbacks:"
```

2. Check router settings:
```bash
cat config/litellm/config.yaml | grep -A 10 "router_settings:"
```

**Expected Results**:
- Fallback chains are defined for key models (gpt-4o, claude-3-5-sonnet, gpt-4o-mini)
- Router strategy is `usage-based-routing-v2`
- `num_retries: 2` and `timeout: 60` are set
- Each fallback chain has 3-4 alternative models

**Simulating a Fallback** (Optional):
To actually test fallback, you can temporarily invalidate an API key:

```bash
# Edit .env and temporarily break OPENAI_API_KEY
nano .env
# Change OPENAI_API_KEY to something invalid

# Restart LiteLLM
docker-compose restart litellm

# Try to use gpt-4o - it should fall back to openrouter/gpt-4o
# Then restore your API key
```

## Test 7: Budget Monitoring

**Goal**: Check that budget tracking is working

**Steps**:
1. View current budget configuration:
```bash
cat config/litellm/config.yaml | grep -A 5 "max_budget"
```

2. Check spending against budget:
```bash
sqlite3 data/litellm/litellm_cost_tracking.db "
SELECT 
    ROUND(SUM(cost), 2) as total_spent,
    100 as budget,
    ROUND((SUM(cost) / 100) * 100, 1) as percent_used
FROM spend_log 
WHERE created_at > strftime('%s', 'now', '-30 days');
"
```

**Expected Results**:
- max_budget is set to $100 (or your custom value)
- budget_duration is "30d"
- Query shows total spending is within budget
- You can calculate remaining budget

**Setting Up Budget Alerts**:
Create a monitoring script:
```bash
#!/bin/bash
# budget_monitor.sh
SPENT=$(sqlite3 data/litellm/litellm_cost_tracking.db "SELECT ROUND(SUM(cost), 2) FROM spend_log WHERE created_at > strftime('%s', 'now', '-30 days');")
BUDGET=100
PERCENT=$(echo "scale=1; ($SPENT / $BUDGET) * 100" | bc)

echo "Current spending: \$$SPENT / \$$BUDGET ($PERCENT%)"

if (( $(echo "$PERCENT > 80" | bc -l) )); then
    echo "âš ï¸  WARNING: Over 80% of budget used!"
fi
```

## Quick Verification Commands

Run these commands to get a quick health check:

```bash
# 1. Check all services are running
docker-compose ps

# 2. Verify LiteLLM is healthy
curl -s http://localhost:4000/health | jq

# 3. List available models
curl -s http://localhost:4000/models | jq '.data[].id'

# 4. Check recent spending
sqlite3 data/litellm/litellm_cost_tracking.db "SELECT COUNT(*) as requests, ROUND(SUM(cost), 4) as total_cost FROM spend_log;" 2>/dev/null || echo "Database not yet created"

# 5. View last 5 requests
sqlite3 data/litellm/litellm_cost_tracking.db "SELECT model, ROUND(cost, 6) as cost, datetime(created_at, 'unixepoch') FROM spend_log ORDER BY created_at DESC LIMIT 5;" 2>/dev/null || echo "No requests logged yet"
```

## Common Issues

### Issue: Models not appearing in Open WebUI

**Solution**:
1. Check LiteLLM is running: `docker-compose ps litellm`
2. Check logs: `docker-compose logs litellm`
3. Restart Open WebUI: `docker-compose restart open-webui`
4. Clear browser cache and reload

### Issue: "Invalid API key" errors

**Solution**:
1. Verify API keys in `.env`: `cat .env | grep API_KEY`
2. Ensure no extra spaces or quotes
3. Restart LiteLLM: `docker-compose restart litellm`
4. Test API key directly with the provider's API

### Issue: Cost tracking database not created

**Solution**:
1. Check data directory exists: `ls -la data/litellm/`
2. Check permissions: `chmod 777 data/litellm/`
3. Restart LiteLLM: `docker-compose restart litellm`
4. Send a test message to trigger database creation
5. Check logs: `docker-compose logs litellm | grep database`

### Issue: Fallback not working

**Solution**:
1. Verify fallback configuration: `cat config/litellm/config.yaml | grep -A 20 fallbacks`
2. Check retry settings are enabled
3. Review logs for retry attempts: `docker-compose logs litellm | grep -i retry`
4. Ensure backup models have valid API keys

### Issue: OpenRouter models fail

**Solution**:
1. Verify OpenRouter API key: `docker-compose exec litellm env | grep OPENROUTER`
2. Check OpenRouter dashboard for remaining credits
3. Test OpenRouter API directly: `curl https://openrouter.ai/api/v1/models -H "Authorization: Bearer $OPENROUTER_API_KEY"`
4. Check model name matches OpenRouter's API names

## Success Criteria

All v1.1 features are working correctly if:

- âœ… All 16 models appear in Open WebUI model selector
- âœ… Can successfully send messages to multiple models
- âœ… Cost tracking database is created and logging requests
- âœ… OpenRouter models work (if API key configured)
- âœ… Temperature, top_p, max_tokens parameters are in config
- âœ… Fallback chains are configured for key models
- âœ… Budget monitoring is active

## Performance Expectations

- **Model Selection**: Instant (client-side)
- **Message Response Time**: 2-10 seconds depending on model
- **Database Write**: < 100ms per request
- **Fallback Activation**: < 5 seconds on primary failure
- **Memory Usage**: LiteLLM ~100-200 MB RAM

## Next Steps

After confirming all smoke tests pass:

1. âœ… Read the [Model Configuration Guide](docs/MODEL_CONFIGURATION.md)
2. âœ… Customize model parameters for your use case
3. âœ… Set up budget monitoring alerts
4. âœ… Add more OpenRouter models as needed
5. âœ… Configure custom fallback chains
6. âœ… Share feedback and use cases!

---

**Happy Testing!** ðŸ§ªðŸš€

For issues or questions, see the [main README](README.md) or check the [logs](#common-issues).
