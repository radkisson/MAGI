# Dynamic OpenRouter Model Loading

## Overview

Starting with this release, RIN automatically fetches and updates the list of available models from the OpenRouter API. This ensures you always have access to the latest models without manual configuration updates.

## How It Works

### Automatic Sync on Startup

When you run `./start.sh`, RIN automatically:
1. Connects to the OpenRouter API
2. Fetches the complete list of available models
3. Filters models based on availability and quality criteria
4. Updates the LiteLLM configuration with fresh model entries
5. Preserves your existing non-OpenRouter models (OpenAI, Anthropic direct)

### Manual Sync

You can manually sync models at any time:

```bash
# Quick sync
./scripts/sync_models.sh

# Or run the Python script directly
python3 scripts/sync_openrouter_models.py
```

After syncing, restart LiteLLM to apply changes:

```bash
docker-compose restart litellm
```

## Configuration

### Prerequisites

1. **OpenRouter API Key** (Required for model access):
   - Get your key from [OpenRouter](https://openrouter.ai/)
   - Add to `.env`:
     ```bash
     OPENROUTER_API_KEY=your_key_here
     ```

2. **Python Requirements** (Installed automatically):
   - `requests` - For API calls
   - `pyyaml` - For config file handling

### Model Filtering

The sync script automatically filters models to include only:

- **Active models** - Not deprecated or removed
- **Available models** - Have pricing information (indicates availability)
- **Base variants** - Excludes extended context variants to reduce clutter
  - Exception: Keeps useful variants like `128k-online` and `sonar` models

## Model Naming

Models are automatically named using a consistent pattern:

- **Format**: `openrouter/{provider}-{model}`
- **Example**: `openrouter/openai-gpt-4o`
- **Original ID preserved** in the `litellm_params.model` field

## Features

### Automatic Capability Detection

The sync script automatically detects and tags models with:

- **Function Calling** - Models that support tool/function calling
- **Vision Support** - Models that can process images
- **Context Length** - Automatically configures max_tokens based on model context

### Fallback to Static Config

If the sync fails (no API key, network issues, etc.), RIN gracefully falls back to:
- Your existing configuration
- Default models in the config file
- No disruption to service

This ensures RIN works even without OpenRouter API access.

## Usage Examples

### Example 1: First Time Setup

```bash
# 1. Add your OpenRouter API key
nano .env
# Add: OPENROUTER_API_KEY=sk-or-v1-...

# 2. Start RIN (auto-syncs models)
./start.sh

# 3. Models are now available in Open WebUI
```

### Example 2: Adding New Models

OpenRouter frequently adds new models. To get them:

```bash
# 1. Sync models
./scripts/sync_models.sh

# 2. Restart LiteLLM
docker-compose restart litellm

# 3. New models appear in Open WebUI immediately
```

### Example 3: Scheduled Sync

Add to crontab for daily model updates:

```bash
# Sync models daily at 3 AM
0 3 * * * cd /path/to/RIN && ./scripts/sync_models.sh && docker-compose restart litellm
```

## Troubleshooting

### Models Not Appearing

**Problem**: Models don't show up in Open WebUI

**Solutions**:
1. Check if sync succeeded:
   ```bash
   ./scripts/sync_models.sh
   ```

2. Verify LiteLLM restarted:
   ```bash
   docker-compose logs litellm | tail -20
   ```

3. Check model list endpoint:
   ```bash
   curl http://localhost:4000/models
   ```

### Sync Fails

**Problem**: `sync_openrouter_models.py` fails with errors

**Common causes**:

1. **No API key**:
   ```bash
   # Check .env has OPENROUTER_API_KEY
   grep OPENROUTER_API_KEY .env
   ```

2. **Network issues**:
   ```bash
   # Test connectivity
   curl https://openrouter.ai/api/v1/models
   ```

3. **Python dependencies**:
   ```bash
   # Install requirements
   pip3 install requests pyyaml
   ```

### Too Many Models

**Problem**: Model list in Open WebUI is overwhelming

**Solution**: Edit the filter criteria in `scripts/sync_openrouter_models.py`:

```python
def filter_models_by_criteria(models: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Custom filtering logic"""
    filtered = []
    
    for model in models:
        model_id = model.get('id', '')
        
        # Add your custom filters here
        # Example: Only include OpenAI and Anthropic
        if not any(x in model_id for x in ['openai', 'anthropic']):
            continue
        
        # Example: Exclude expensive models
        pricing = model.get('pricing', {})
        if float(pricing.get('prompt', 0)) > 0.00005:
            continue
        
        filtered.append(model)
    
    return filtered
```

## Advanced Configuration

### Custom Model Parameters

After sync, you can customize individual models in `config/litellm/config.yaml`:

```yaml
model_list:
  # Auto-generated by sync
  - model_name: openrouter/openai-gpt-4o
    litellm_params:
      model: openrouter/openai/gpt-4o
      api_key: os.environ/OPENROUTER_API_KEY
      temperature: 0.7  # Customize this
      max_tokens: 4096  # Customize this
      top_p: 1.0
    model_info:
      mode: chat
      supports_function_calling: true
```

### Preserve Custom Models

The sync script automatically preserves:
- Direct API models (OpenAI, Anthropic)
- Custom model configurations you've added

Only OpenRouter models are updated during sync.

### Disable Auto-Sync

To disable automatic sync on startup:

1. Comment out the sync section in `start.sh`:
   ```bash
   # --- 5.1 SYNC OPENROUTER MODELS ---
   # echo -e "${BLUE}ðŸ”„ Synchronizing OpenRouter models...${NC}"
   # if [ -f "$BASE_DIR/scripts/sync_openrouter_models.py" ]; then
   #     python3 "$BASE_DIR/scripts/sync_openrouter_models.py" 2>/dev/null || {
   #         echo "âš ï¸  Could not fetch latest models"
   #     }
   # fi
   ```

2. You can still manually sync:
   ```bash
   ./scripts/sync_models.sh
   ```

## Architecture

### Sync Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OpenRouter API â”‚
â”‚  /api/v1/models â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ sync_openrouter_models â”‚
â”‚   - Fetch models       â”‚
â”‚   - Filter by criteriaâ”‚
â”‚   - Convert to LiteLLM â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ config/litellm/       â”‚
â”‚ config.yaml           â”‚
â”‚   - Preserve existing â”‚
â”‚   - Add OpenRouter    â”‚
â”‚   - Backup old config â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LiteLLM Service     â”‚
â”‚   Reload config       â”‚
â”‚   Serve models        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### File Structure

```
scripts/
â”œâ”€â”€ sync_openrouter_models.py  # Core sync logic
â””â”€â”€ sync_models.sh             # Convenience wrapper

config/litellm/
â”œâ”€â”€ config.yaml                # Active config
â””â”€â”€ config.yaml.backup         # Auto-backup
```

## API Reference

### OpenRouter Models Endpoint

- **URL**: `https://openrouter.ai/api/v1/models`
- **Method**: `GET`
- **Auth**: Bearer token (optional, public data available without auth)
- **Response**: JSON with model list

### Response Format

```json
{
  "data": [
    {
      "id": "openai/gpt-4o",
      "name": "GPT-4o",
      "pricing": {
        "prompt": "0.0000025",
        "completion": "0.00001"
      },
      "context_length": 128000,
      "architecture": {
        "modality": "text+image->text",
        "tokenizer": "GPT",
        "instruct_type": "none"
      },
      "supported_parameters": ["temperature", "top_p", "max_tokens"],
      "description": "..."
    }
  ]
}
```

## Best Practices

1. **Sync Regularly**: Run sync weekly or when you need new models
2. **Review Config**: Check `config.yaml.backup` if something goes wrong
3. **Test New Models**: Try new models with simple queries first
4. **Monitor Costs**: New models may have different pricing
5. **Keep Fallbacks**: Maintain direct API models for reliability

## Security Notes

- API keys are stored in `.env` (not committed to git)
- Sync script never exposes keys in logs
- Backup files may contain sensitive config (excluded via `.gitignore`)

## Future Enhancements

Planned improvements:
- [ ] Model popularity rankings
- [ ] Cost-based filtering
- [ ] Model capability search
- [ ] Automatic model recommendations
- [ ] Performance benchmarking integration

## Support

For issues or questions:
- GitHub Issues: [Report a bug](https://github.com/radkisson/Rhyzomic-Intelligence-Node-RIN-/issues)
- Documentation: [Full docs](../README.md)

---

**Dynamic Models** - Always up-to-date, always ready. ðŸš€
