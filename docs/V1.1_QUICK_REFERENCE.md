# v1.1 Enhanced Model Support - Quick Reference

This is a quick reference card for the v1.1 "Expansion" features. For detailed documentation, see [MODEL_CONFIGURATION.md](MODEL_CONFIGURATION.md).

## ðŸš€ New Features at a Glance

### 1. OpenRouter Integration
Access 20+ models through a single API:
- **OpenAI**: GPT-4o, GPT-4 Turbo
- **Anthropic**: Claude 3.5 Sonnet, Claude 3 Opus
- **Meta**: Llama 3.1 405B, Llama 3.1 70B
- **Google**: Gemini Pro 1.5, Gemini Flash 1.5
- **Mistral**: Mistral Large, Mixtral 8x7B
- **Cohere**: Command R+
- **Perplexity**: Web-connected models

**Setup**: Add `OPENROUTER_API_KEY` to `.env` and restart

### 2. Advanced Model Parameters
Fine-tune each model's behavior:
```yaml
temperature: 0.7    # Creativity (0.0-2.0)
top_p: 1.0         # Diversity (0.0-1.0)
max_tokens: 4096   # Response length
```

**Defaults**: All models pre-configured with balanced settings

### 3. Model Selection UI
All models automatically appear in Open WebUI:
- Direct API models
- OpenRouter variants
- Visual model selector dropdown
- Switch models mid-conversation

**Access**: http://localhost:3000 â†’ Model dropdown

### 4. Cost Tracking & Budgeting
Monitor spending in real-time:
```bash
# View costs
sqlite3 data/litellm/litellm_cost_tracking.db
> SELECT model, SUM(cost) FROM spend_log GROUP BY model;
```

**Budget**: Default $100/month (configurable in `config.yaml`)

### 5. Fallback Chains
Automatic failover on model errors:
```
gpt-4o â†’ openrouter/gpt-4o â†’ claude-3-5-sonnet â†’ llama-3.1-405b
```

**Benefits**: 99.9% uptime, multi-provider redundancy

## ðŸ“‹ Quick Start

### Minimal Setup
1. Run `./start.sh` (creates all required directories)
2. Add at least one API key to `.env`:
   - `OPENAI_API_KEY=your_key`
   - OR `ANTHROPIC_API_KEY=your_key`
   - OR `OPENROUTER_API_KEY=your_key`
3. Visit http://localhost:3000
4. Select a model and start chatting

### Full Setup (All Features)
1. Get API keys:
   - OpenAI: https://platform.openai.com/api-keys
   - Anthropic: https://console.anthropic.com/
   - OpenRouter: https://openrouter.ai/keys
2. Add to `.env`:
   ```bash
   OPENAI_API_KEY=sk-...
   ANTHROPIC_API_KEY=sk-ant-...
   OPENROUTER_API_KEY=sk-or-...
   ```
3. Restart: `docker-compose restart litellm`
4. All 16 models are now available!

## ðŸ”§ Common Tasks

### Add a New Model
Edit `config/litellm/config.yaml`:
```yaml
model_list:
  - model_name: my-model
    litellm_params:
      model: openrouter/provider/model-name
      api_key: os.environ/OPENROUTER_API_KEY
      temperature: 0.7
      max_tokens: 4096
```
Restart: `docker-compose restart litellm`

### Adjust a Model's Temperature
Edit the model's `temperature` in `config.yaml`:
- `0.0-0.3`: Precise, focused (good for code, facts)
- `0.7-0.9`: Balanced (default, good for general use)
- `1.0-2.0`: Creative, varied (good for writing, brainstorming)

### Check Spending
```bash
# Total spending
sqlite3 data/litellm/litellm_cost_tracking.db \
  "SELECT ROUND(SUM(cost), 2) FROM spend_log;"

# Per-model breakdown
sqlite3 data/litellm/litellm_cost_tracking.db \
  "SELECT model, ROUND(SUM(cost), 2) as cost, COUNT(*) as requests 
   FROM spend_log GROUP BY model ORDER BY cost DESC;"
```

### Set Budget Limit
Edit `config/litellm/config.yaml`:
```yaml
general_settings:
  max_budget: 50  # USD per month
  budget_duration: "30d"
```

### Configure Fallback Chain
Edit `router_settings.fallbacks` in `config.yaml`:
```yaml
fallbacks:
  - my-model:
    - primary-model
    - backup-model-1
    - backup-model-2
```

## ðŸ“Š Model Recommendations

### Best for Code
1. `claude-3-5-sonnet` - Best overall
2. `gpt-4o` - Strong reasoning
3. `openrouter/llama-3.1-405b` - Open source

### Best for Cost
1. `gpt-4o-mini` - Fast and cheap
2. `claude-3-5-haiku` - Budget Claude
3. `openrouter/mixtral-8x7b` - Free tier available

### Best for Long Context
1. `claude-3-5-sonnet` - 200K tokens
2. `openrouter/gemini-pro` - 128K tokens
3. `gpt-4o` - 128K tokens

### Best for Vision
1. `gpt-4o` - Excellent vision
2. `openrouter/gemini-pro` - Strong vision
3. `claude-3-opus` - Good vision

### Best for Research
1. `openrouter/perplexity-online` - Web-connected
2. `gpt-4o` - Strong reasoning
3. `claude-3-5-sonnet` - Detailed analysis

## ðŸŽ¯ Configuration Files

```
config/litellm/config.yaml    # Model definitions
.env                           # API keys
docker-compose.yml             # Service orchestration
data/litellm/                  # Cost tracking database
```

## ðŸ“š Documentation

- **Full Guide**: [MODEL_CONFIGURATION.md](MODEL_CONFIGURATION.md)
- **Smoke Tests**: [V1.1_SMOKE_TESTS.md](V1.1_SMOKE_TESTS.md)
- **Main README**: [../README.md](../README.md)
- **Changelog**: [../CHANGELOG.md](../CHANGELOG.md)

## ðŸ†˜ Troubleshooting

### Models not appearing?
```bash
docker-compose logs litellm | grep -i error
docker-compose restart open-webui
```

### API errors?
```bash
# Check keys
cat .env | grep API_KEY

# Test LiteLLM
curl http://localhost:4000/health
```

### Database not working?
```bash
# Check directory
ls -la data/litellm/

# Fix permissions
chmod 777 data/litellm/
docker-compose restart litellm
```

## ðŸ’¡ Pro Tips

1. **Use OpenRouter for variety**: Access 100+ models with one API key
2. **Set conservative budgets**: Start with $10-20/month, adjust as needed
3. **Monitor costs weekly**: Run `sqlite3 data/litellm/...` queries
4. **Configure fallbacks**: Always have backup models
5. **Test temperature variations**: Different tasks need different settings
6. **Cache responses**: LiteLLM automatically caches via Redis
7. **Use cheap models for testing**: gpt-4o-mini is great for development

## ðŸ“ˆ Cost Examples (Approximate)

| Task | Model | Est. Cost |
|------|-------|-----------|
| Quick question (100 tokens) | gpt-4o-mini | $0.0001 |
| Chat conversation (1K tokens) | claude-3-5-haiku | $0.002 |
| Code generation (4K tokens) | claude-3-5-sonnet | $0.05 |
| Long document (50K tokens) | gpt-4o | $0.50 |

## ðŸ”— Useful Links

- [OpenRouter Models](https://openrouter.ai/models) - Browse available models
- [LiteLLM Docs](https://docs.litellm.ai/) - Advanced configuration
- [Open WebUI](http://localhost:3000) - Your RIN interface
- [LiteLLM API](http://localhost:4000) - Direct API access

---

**v1.1 "Expansion"** - More models, more control, more reliability. ðŸ§ âœ¨
